% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{llncsdoc}
%
\begin{document}
\markboth{\LaTeXe{} Class for Lecture Notes in ComputerScience}
{\LaTeXe{} Class for Lecture Notes in Computer Science}
\thispagestyle{empty}
\vspace{2pt}
\vfill

%
\title {Leveraging  Aging theory in topic-focused multi-document timeline summarization} 
\maketitle
%
\section{abstract}

Topic-focused multi-document summarization plays an important role in helping readers to get the main information from any topic. Many approaches are proposed to generate the timeline summarization, but seldom consider the life circle of each topic. In this paper, we leveraging aging theory to present the sentence feature, and train the classification model with the SMOTEBoost technology. We also evaluate our approach on two corpus, one of which is a public data set, the other one is our manual annotation data set. Experiment results show that our method can improve the timeline summarization significantly. 


%
\section{Introduction}
%


Everyday thousands of news stories reporting different events are published on the Internet. These reports are disordered and people have to read most of them to know what is happening which is a time-consuming job undoubtedly. How can we get useful information about an event efficiently? Automatic summarization has been such a method solving this kind of information overloading since Luhn \cite{1958-Luhn-p159-165} proposed it in 1958. And numerous pages have been published in the field, ranging from single document to multiple documents, from extraction to abstraction, from traditional document to web document, email, blog and other types of genre. However, these research work focus on the central idea of document or document set ignoring the temporal characteristics of events. As a result, people cannot catch the changes of events over time efficiently. 

Recent years, topic detection and tracking (TDT) which detects new events from the large scale news stream and tracks them as events going on draws researchers' attention. But it did not display events properly, and people still have to read all the relevant reports to get what they want to know about the event. However, we are still enlightened by its usage of tracking which make us decide to generate a timeline summary consisting of a series of individual small summaries with sentences both important and diverse to help people understand the development of an event more quickly.

Every event goes through a life cycle of birth, growth, maturity and death, which means that special terms utilized for descripting different events experience a similar life cycle. Aging theory \cite{2003-Chen-p47-59} is a model exploited in event detection task which tracks life cycles of events using energy function. The energy of an event increases when the event becomes popular, and it diminishes with time. In our opinion, it can also be used for summarization to help us find out the daily hot terms of events. Then people can obtain what new changes happen as events going on.

The importance of sentences is decided by terms occurring at the documents in keywords-based summarization. But different authors use different words to express a same meaning and many words has several meanings. So identifying the implicit semantics of news can improve summary quality greatly. Here, we propose an incremental model based on latent semantic analysis (LSA) \cite{1990-Deerwester-p391-407} which is a robust unsupervised technique for deriving an implicit representation of text semantics based on observed co-occurrence of words to find semantic units of news.

As descripted above, in this paper, we generate news event summary by considering both temporal and semantic characteristics. We first utilize the aging theory \cite{2003-Chen-p47-59} to extract hot terms from news which reports the same event according to their energy during the time interval we choose (i.e. one day). Then we identify the semantic units of news with the incremental latent semantic analysis model. Last, we construct a semantic text relationship map, choose sentences which are both important and novel to form the summary and display them using a timeline so that people can track event trajectory easily and quickly.

The remainder of this paper is organized as follows: Section 2 reviews some related works on summarization. We discuss our approach of event timeline summarization using aging theory and incremental latent semantic analysis model in section 3. Our experiments and some discusses are described in section 4. Section 5 presents our conclusions and some future plans.


%
\section{Related Work}
%
\textbf{Topic-focused multi-document summarization} aims at gain main information from multiple text about the same topic. There are two ways to achive this goal, one is extract important sentences, the other one is build new sentence to express the key idea. In this paper, we focuse on the former method.

One of the most popular extractive multi-document summarization method is MEAD, which take term frequency, sentence position, first-sentence overlap to present the feature of each sentece. Giang Binh Tran\cite{tran2013leveraging} investigate five different sentence feature and leverage SVMRank to optimize the summarization task. 

\textbf{Timeline summarization} is 

\textbf{Aging thory}




%
\section{Our Approach}
%

\subsection{Sentence Feature Selection}

In order to represent sentence, we extract five kinds of features as flows:

\textbf{Surface feature}: this contains features computed by basic statistics, such as the length of sentence, the counts of nourm words and stop words, the position in this document and parapraf, and wheather it contains person name or not.

\textbf{Importance feature}: this feature aim to respesent the impantance of this sentence. The weight of sentence is computed through linear combination of term weights with latem senmatic analysis.

\textbf{Aging feature}: We use this feature to show the life cycle of this sentence.

\textbf{Noviety feature}: 

\textbf{Topic feature}: 


\subsection{Model Trainning}

With the help of labled data, we convert this summarization task to pairwise classification problem. The positive data is sentences labeld to summary, otherwise is negative. 

Because the count of summary sentence is much less than the normal sentence, the train data set is unblanced. In order to reduce this reflect, SMOTEBoost method is used to train the classification model.



%
\section{Evaluation}
%

\subsection{Evaluation metric}

Rogue

Prcesesiction

\subsection{Experiment on public data set}

The public data set form Giang Binh Tran\cite{tran2013leveraging} is used in this research.  



\subsection{Experiment on muannl labeld data}




%
\section{Future}
%


\bibliographystyle{ieeetr}
\bibliography{../../bib/text_summary}


\end{document}