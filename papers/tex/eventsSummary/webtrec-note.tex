
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{url}
%\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
%\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
%\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
%\newcommand{\keywords}[1]{\par\addvspace\baselineskip
%\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{DLDE at web track: ad-hoc task}

% a short form should be given in case it is too long for the running head
\titlerunning{Ad-hoc task note}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Jie Chen \and Yulong Shi \and Changmin Zhang \and Weiyin Li \and Zhendong Niu}

%
\authorrunning{Notebook to ad-hoc task of web track 2013}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
%\institute{Springer-Verlag, Computer Science Editorial,\\
%Tiergartenstr. 17, 69121 Heidelberg, Germany\\
%\mailsa\\
%\mailsb\\
%\mailsc\\
%\url{http://www.springer.com/lncs}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle

\begin{abstract}
\emph{In this note paper, we report our experiment method at ad-hoc task of Web Track 2013. The goal of this task is to return a ranking of the documents order by relevance from the collection of static web pages. Our group use meta search to help query expansion as the first step , then retrieval with the expansion query to get the search results and rerank them. }
\keywords{Ad-hoc search, query expansion , meta search}
\end{abstract}

\section{Introduction}

The ad-hoc task of web track is based on some given topics to search out the most relevant documents from a large number of static web pages, ClueWeb2012 \cite{clueweb2012}, which comprises about 870 million web pages, collected between February 10, 2012 and May 10, 2012. Topics are similar to queries of tradition web search, short and ambiguity. It's hard to get better search results based on keywords match method. This year we use the WebClue2012-B13 dataset , a subset of WebClue2012 with 50 million web pages.Firstly, we do some prepare word to the dataset in order to remove the spams and noise data. Secondly, in order to get more semantic meanings, we use meta search approach to get some pages relevant to the topic as seeds , then we compute the semantic words as expansion. Thirdly, we rerank the search results from the treated data with the expansion query. 

\section{Data preprocessing}

Since the data set is too big to operate directly and efficiently, we remove the non-relevant  data and spam before doing the final query step. In our experiments , we first use the Indri \cite{indri} tools and waterloo spam \cite{2011-Cormack-p441-465} to build the raw index files. Second, we get all the relevant pages from the index with the topic as the query. These relevant pages are the basic data of our experiments. In the process of parsing the web page , we find here are many noises    in body such as advertisements, copy rights, so the Block web content parser \cite{2012-Lin-p256-264} , developed by our lab,  is introduced into this project to extract the main content. Third, we build a new index file with Lucene \cite{lucene} for subsequent experiments.  The reason is that our group has developed a web search system based on Lucene package.

\section{Query Search}

We divided the query search phase into two parts: query expansion and reranking. We expand each origin topic to be a sematic word set as new query to get search results and treate them as the final results after reranking. 

\subsection{Query Expansion }

\subsubsection{Meta search}

We use google search and bing search as a meta search resource. Each search engine return the top 200 pages about the topic, then we use  the page extract technology \cite{2012-Lin-p256-264} to get main content.  

\subsubsection{Expansion Stractagcy}
Query expansion is a commonly used method help search system to understand the origin query words. In our experiment, we use the local analysis \cite{2010-Imran-p386-393} method to get the expansion words. By calculating occurrence number of each word and remove the stop words to get the orgin expansion words list. With the help of Standford Parser \cite{standfordparser} , developed by Standford Natural Language Processing Group, we remove all the words in addition to nouns and get the top 30 as the final expansion words list.

We treate the synonym of origin topic word as the denominator to get the weight of each expansion word
\begin{equation}
w_i = \frac{TF_i}{TF_{max}}
\end{equation}
The final query expansion formula is descripted : 
\begin{equation}
Query_{expan} = Query_{origin} + \sum_{i=1}^n w_i * Expan_i
\end{equation}
$n$ is the count of optional expansion terms.
\subsection{Re-ranking Model}

We aim to re-ranking the search results with learning to rank technology, on the rise in recent years. Learning to rank is a class of methods used machine learning to solove information retrieval problems. It is an effective way to combine different data features for ranking. We use the public data available for learning to rank, LETOR \cite{2007-Liu-p3-10}. Since the LETOR has a lot of features but we can get less,  we deleted those features of LETOR then train the ranking model. We use the SVMRank \cite{2010-Chapelle-p201-215} algorithm in this task and do five-folds cross validation. The output model is directly applied to the experiment.

\section{Experiments}

This year we submit only one run and it performs not good. We think there are two main reasons for this result. One reason is our ClueWeb-B-13 dataset is too small to obtain the valid search result in the data preprocessing step. The other  is features we used to train the ranking model .Not only the counts of features is a small number ,but there are gaps between LETOR dataset and ClueWeb2012.We need to do some transfer learning  to train the ranking model next time.

\section{Conculion}

Our approach is descripted in this paper. This year we use web content technology to remove the noise of web page and use the meta search and query expansion method to understand the topic. Since the dataset we have is only one tenth of the whole ,  the rate of two evaluation criteria are low. In the future , we will use the whole dataset of ClueWeb2012 to validate our ideas.

\section{Acknowledgements}

Thank organizers of TREC and NIST.


\begin{thebibliography}{4}
\bibitem{2010-Imran-p386-393}Imran, H. ; Sharan, A. A framework for automatic query expansion Web Information Systems and Mining, Springer, 2010, 386-393
\bibitem{2012-Lin-p256-264}Lin, S.; Chen, J. ; Niu, Z. Combining a segmentation-like approach and a density-based approach in content extraction Tsinghua Science and Technology, TUP, 2012, 17, 256-264
\bibitem{2007-Liu-p3-10}Liu, T.-Y.; Xu, J.; Qin, T.; Xiong, W.; Li, H. Letor: Benchmark dataset for research on learning to rank for information retrieval Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval, 2007, 3-10
\bibitem{2010-Chapelle-p201-215}Chapelle, O.; Keerthi, S. S. Efficient algorithms for ranking with SVMs Information Retrieval, Springer, 2010, 13, 201-215
\bibitem{clueweb2012} Carnegie Mellon University. The ClueWeb2012 Dataset [EB.OL]. http://boston.lti.cs.cmu.edu/clueweb12/
\bibitem{indri} The Lemur Project. The Indri search engine software. http://lemurproject.org/indri.php
\bibitem{lucene} The ApacheSoftware Foundation. The Lucene Search Library. http://lucene.apache.org/
\bibitem{standfordparser} The Standford Natural Language processing Group. The Standford Parser. http://nlp.stanford.edu/software/lex-parser.shtml
\end{thebibliography}



\end{document}
